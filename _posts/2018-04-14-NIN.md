---
layout: post
title:  NIN
categories: 经典CNN
tags:  CV
mathjax: true
---

* content
{:toc}

[Network In Network](http://xueshu.baidu.com/s?wd=paperuri%3A%28b141af8f73398aaf072b958f50807e1b%29&filter=sc_long_sign&tn=SE_xueshusource_2kduw22v&sc_vurl=http%3A%2F%2Farxiv.org%2Fpdf%2F1312.4400&ie=utf-8&sc_us=13783729042765531986)
是2013年的一篇论文，创新点在于mlpconv(multi-layer perception conv)和global average pooling。





## MLPCONV

<div align="center"><img src="/photoes/2018/mlpconv.png" /></div>

传统的卷积类似于先提取线性特征，再判断特征是否激活，所以提取抽象特征的能力不够，若使用mlpconv可以提取更加抽象的
特征，且可以是非线性组合。mlpconv中的激活函数任然使用Relu，反向传播过程和一般网络类似。据原文推测，作者是受到maxout网络
的启发，maxout是对隐藏层的输出取最大值，可以参考[这里](https://blog.csdn.net/xbinworld/article/details/61210499),maxout的
拟合能力非常强，可以拟合任意的凸函数。

## GAP

NIN去除了传统的全连接层，因为全连接层类似一个黑盒，参数很多容造成过拟合。NIN用gloabal average pooling。

<div align="center"><img src="/photoes/2018/global_average_pooling.png" /></div>

在卷积层的最后一层产生与类别数目一样多的feature map作为该类的证据，对每个feature map取平均，接softmax层。

实验结果表明，global average pooling是一种有效的正则方法。

## 实验

用4层的nin对cifar-1000数据集分类，效果好于Alexnet，参数量约为Alexnet的1/10。

<div align="center"><img src="/photoes/2018/nin.png" /></div>

## 1*1卷积核

单通道1*1卷积等价于原始feature map乘以一个常量，但是多通道1*1卷积可以实现通道间的特征线性组合，我觉得和其他尺寸的卷积相比
只是感受野不同，提取抽象特征的本质是一样的。

在GoogLeNet中可以降维，先使用1*1卷积核降低通道数，再进行其他尺寸的降维，极大减小了参数数量和运算量。

全连接层到全连接层可以看作1*1的卷积层。

<div align="center"><img src="/photoes/2018/mlpconv.png" /></div>






