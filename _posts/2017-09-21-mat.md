---
layout: post
title:  向量內积 SVD分解
categories: 机器学习
tags:  向量內积  矩阵投影
mathjax: true
---

* content
{:toc}

向量內积即$\vec a$在向量$\vec b$上的投影长度与$\vec b$长度的乘积，若$\vec b$长度为1， 则向量內积结果就是投影长度；矩阵与向量乘积，即该向量在不同向量上的投影。特征值分解和奇异值分解
之间的联系与区别。





## 矩阵和向量乘积

$$
\begin{pmatrix} a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\ a_{21} & a_{22} & a_{23} & \cdots & a_{2n} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & a_{m3} & \cdots & a_{mn} \end{pmatrix} 
\cdot \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}  =   \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix} 
$$

所得向量$\vec y$是$\vec a_1, \vec a_2, \vec a_n$的线性组合。其实矩阵乘法最基本的意义就是线性组合。

其实就是多维空间下的计算，列向量表示多维空间中的一个点，矩阵表示多维空间中选取的坐标系，所得结果就是在所选取坐标系下的坐标表示。

## 特征值分解和奇异值分解

[一篇很好的参考文章](http://blog.csdn.net/redline2005/article/details/24100293),我的理解是特征值分解是奇异值分解的特列，
其实所谓的变化只是改变选取的坐标系(原始坐标系即二维、三维坐标系的类推)。(不知道过些时间，自己写的话还能不能看懂。。。。)

$$
M  = U \sum V^T
$$

$$
M \mathrm x = \sum \limits_i  \mathrm u_i \sigma_i \mathrm v_i^T \mathrm x
$$

其中$\mathrm u_i$是变换前选取的坐标系，$\mathrm v_i$是变换后选取的坐标系。若把$\mathrm x$表示为$\mathrm u_i$的线性组合。其实矩阵乘法最基本的意义就是线性组合，那么$M \mathrm x $表示在$\mathrm v_i$坐标系下的坐标。
且变换为伸缩变换。

## 复矩阵

复数可以理解为伸缩和角度两种变换



